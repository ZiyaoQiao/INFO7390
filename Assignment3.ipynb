{"cells":[{"cell_type":"code","source":["import pandas as pd\nimport matplotlib.pyplot as plt\nimport math\nimport seaborn as sns\n\nfrom functools import partial\nfrom multiprocessing import Pool\n\nfrom PIL import Image"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import numpy as np\nimport tensorflow as tf\nimport os\nfrom tensorflow.python.platform import gfile\nimport os.path\nimport re\nimport sys\nimport tarfile\nfrom subprocess import Popen, PIPE, STDOUT\ndef run(cmd):\n  p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)\n  return p.stdout.read()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import urllib.request\ndef cbk(a,b,c):\n  per = 100.0*a*b/c\n  if(per >100):\n    per = 100\n  print(per)\nurllib.request.urlretrieve(\"http://45.32.248.250:8000/train.zip\", \"/tmp/train.zip\",cbk)\nurllib.request.urlretrieve(\"http://45.32.248.250:8000/test.zip\", \"/tmp/test.zip\",cbk)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["dbutils.fs.mv(\"file:/tmp/train.zip\", \"dbfs:/tmp/sample/train.zip\")\ndbutils.fs.mv(\"file:/tmp/test.zip\", \"dbfs:/tmp/sample/test.zip\")\ndisplay(dbutils.fs.ls(\"dbfs:/tmp/sample\"))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["display(dbutils.fs.ls(\"dbfs:/tmp/sample/train.zip_files/train/\"))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["import zipfile  \ndef un_zip(file_name):  \n    \"\"\"unzip zip file\"\"\"  \n    zip_file = zipfile.ZipFile(file_name)  \n    if os.path.isdir(file_name + \"_files\"):  \n        pass  \n    else:  \n        os.mkdir(file_name + \"_files\")  \n    for names in zip_file.namelist():  \n        zip_file.extract(names,file_name + \"_files/\")  \n    zip_file.close()  \nun_zip('/dbfs/tmp/sample/train.zip')"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["un_zip('/dbfs/tmp/sample/test.zip')"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["train_df = pd.read_csv(\"/dbfs/FileStore/tables/train.csv\")\ndbutils.fs.mkdirs(\"/tmp/sample/json/\")\ntrain_df.head()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["def mkSubFile(lines,head,srcName,sub):  \n    [des_filename, extname] = os.path.splitext(srcName)  \n    filename  = des_filename + '_' + str(sub) + extname  \n    print( 'make file: %s' %filename)  \n    fout = open(filename,'w')  \n    try:  \n        fout.writelines([head])  \n        fout.writelines(lines)  \n        return sub + 1  \n    finally:  \n        fout.close()  \n  \ndef splitByLineCount(filename,count):  \n    fin = open(filename,'r')  \n    try:  \n        head = fin.readline()  \n        buf = []  \n        sub = 1  \n        for line in fin:  \n            buf.append(line)  \n            if len(buf) == count:  \n                sub = mkSubFile(buf,head,filename,sub)  \n                buf = []  \n        if len(buf) != 0:  \n            sub = mkSubFile(buf,head,filename,sub)     \n    finally:  \n        fin.close()  \n  \nif __name__ == '__main__':  \n    splitByLineCount('/dbfs/FileStore/tables/train.csv',1000)  "],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["#Stuructured Streaming"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\ninputPath = \"dbfs:/FileStore/tables/\"\n\n# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\ncsvSchema = StructType([ StructField(\"name\", StringType(), True), StructField(\"type\", StringType(), True) ])\n\n# Static DataFrame representing data in the JSON files\nstaticInputDF = (\n  spark\n    .read\n    .schema(csvSchema)\n    .option(\"header\",\"true\")\n    .csv(inputPath)\n)\n\ndisplay(staticInputDF)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["staticCountsDF = (\n  staticInputDF\n    .groupBy(\n       staticInputDF.type\n    )    \n    .count()\n)\nstaticCountsDF.cache()\n\n# Register the DataFrame as table 'static_counts'\nstaticCountsDF.createOrReplaceTempView(\"static_counts\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%sql select type, sum(count) as total_count from static_counts group by type"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\nstreamingInputDF = (\n  spark\n    .readStream                       \n    .schema(csvSchema)               # Set the schema of the JSON data\n    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n    .csv(inputPath)\n)\n\n# Same query as staticInputDF\nstreamingCountsDF = (                 \n  streamingInputDF\n    .groupBy(\n      streamingInputDF.type\n    )\n    .count()\n)\n\n# Is this DF actually a streaming DF?\nstreamingCountsDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from time import sleep\nsleep(5)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%sql select type as time, count from counts order by type"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["sleep(5)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%sql select type as time, count from counts order by type"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["sleep(5)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%sql select type as time, count from counts order by type"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["%sql select type, sum(count) as total_count from counts group by type order by type"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["For this fixed two-column dataset, I will put csv as a static dataframe. We also have another way to input the csv, and register datagframe as “count”. Features and distribution of dataset could be easily attached. We found that the dataset distributed unevenly, and most of categories have limited photo as the training set. It would be difficult for us to set a model and implement deep learning."],"metadata":{}},{"cell_type":"markdown","source":["#Deep Learning with Apache Spark and TensorFlow"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom glob import glob\nfrom PIL import Image\nimport matplotlib.pylab as plt\n\ntrain_images = glob('/dbfs/tmp/sample/train.zip_files/train/*jpg')\ndf = pd.read_csv('/dbfs/FileStore/tables/train.csv')\n\ndf[\"Image\"] = df[\"Image\"].map( lambda x : \"/tmp/sample/train.zip_files/train/\"+x)\nImageToLabelDict = dict( zip( df[\"Image\"], df[\"Id\"]))\ndf.to_csv('/dbfs/tmp/sample/file.csv')\ndf[\"Image\"] = df[\"Image\"].map( lambda x : \"/dbfs\"+x)\ndf.to_csv('/dbfs/tmp/sample/filewithdbfs.csv')"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["from keras.applications import InceptionV3\nimport h5py\nmodel = InceptionV3(weights=\"imagenet\")\nmodel.compile(optimizer='rmsprop', loss='mse')\nmodel.save('/tmp/model-full.h5')"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["from sparkdl import readImages, DeepImagePredictor\nfrom sparkdl.image import imageIO as imageIO\n\ntrain_images_df = imageIO.readImages(\"/tmp/sample/train.zip_files/train/\")\ntrain_df = train_df.withColumnRenamed('Image','filepath')\ntrain_df = train_df.join(train_images_df,'filepath')\ntrain_df.show()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from keras.applications.inception_v3 import preprocess_input\nfrom keras.preprocessing.image import img_to_array, load_img\nimport numpy as np\nimport os\nfrom sparkdl import KerasImageFileTransformer\nfrom pyspark.sql.types import StringType\ndef loadAndPreprocessKerasInceptionV3(uri):\n    # this is a typical way to load and prep images in keras\n    image = img_to_array(load_img(uri, target_size=(64, 64)))\n    image = np.expand_dims(image, axis=0)\n    return preprocess_input(image)\n\ntransformer = KerasImageFileTransformer(inputCol=\"Image\", outputCol=\"predictions\",\n                                        modelFile=\"/tmp/model-full.h5\",\n                                        imageLoader=loadAndPreprocessKerasInceptionV3,\n                                        outputMode=\"vector\")\n\ndirpath = \"/dbfs/tmp/sample/train.zip_files/train/\"\nfiles = [os.path.abspath(os.path.join(dirpath, f)) for f in os.listdir(\"/dbfs/tmp/sample/train.zip_files/train/\") if f.endswith('.jpg')]\nuri_df = sqlContext.createDataFrame(files, StringType()).toDF(\"Image\")\ncsv_df = spark.read.csv(\"/tmp/sample/filewithdbfs.csv\",header=True)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder, StringIndexer\n\ncsv_df = spark.read.csv(\"/tmp/sample/filewithdbfs.csv\",header=True)\nstringIndexer = StringIndexer(inputCol=\"Id\", outputCol=\"categoryIndex\")\nindexed_dateset = stringIndexer.fit(csv_df).transform(csv_df)\nencoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\nimage_dataset = encoder.transform(indexed_dateset)\nimage_dataset.show(5,False)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["from sparkdl.estimators.keras_image_file_estimator import KerasImageFileEstimator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\ndef loadAndPreprocessKerasInceptionV3(uri):\n    # this is a typical way to load and prep images in keras\n    image = img_to_array(load_img(uri, target_size=(64, 64)))\n    image = np.expand_dims(image, axis=0)\n    return preprocess_input(image)\n\n\n\"\"\"def load_image_and_process(uri):\n  import PIL.Image\n  from keras.applications.imagenet_utils import preprocess_input\n  original_image = PIL.Image.open(uri).convert('RGB')\n  resized_image = original_image.resize((64, 64), PIL.Image.ANTIALIAS)\n  image_array = np.array(resized_image).astype(np.float32)\n  image_tensor = preprocess_input(image_array[np.newaxis, :])\n  return image_tensor\"\"\"\n\nmyEstimator = KerasImageFileEstimator(inputCol='Image',outputCol='CategoryIndex', modelFile='/tmp/model-full.h5',imageLoader=loadAndPreprocessKerasInceptionV3, kerasFitParams={\"epochs\": 5, \"batch_size\": 64})\n"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nkerasParams1 = {'batch_size':10, 'epochs':10}\nkerasParams2 = {'batch_size':5, 'epochs':20}\n\nmyParamMaps = \\\n  ParamGridBuilder() \\\n    .addGrid(myEstimator.kerasFitParams, [kerasParams1, kerasParams2]) \\\n    .build()\n\ncv = CrossValidator(estimator = myEstimator, evaluator = MulticlassClassificationEvaluator, estimatorParamMaps = myParamMaps)\ncvModel = cv.fit(image_dataset)\nkerasTransformer = cvModel.bestModel  # of type KerasTransformer"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["We had tried many different ways to fit the dataset with the existing model in Apache Spark, unfortunately, most of them could not complete the process perfectly. They always face some problems at some specific points, including miss some specific type of data, cannot extract any features from our dataset or incompatible with multi-thread. Pickle seems not work so after refer to professor, we will leave the exsiting running result here and turn to traditional way to implement our neutral network. The below descripes our progress. Because of the performance provided by community edition of databricks and the file number limitation, I will actually run locally instead of in databricks."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom glob import glob\nfrom PIL import Image\nimport matplotlib.pylab as plt\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom subprocess import check_output\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\nfrom keras.preprocessing.image import ImageDataGenerator"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["train_images = glob(\"/dbfs/tmp/sample/train.zip_files/train/*jpg\")\ndf = pd.read_csv(\"/dbfs/FileStore/tables/train.csv\")\n\ndf[\"Image\"] = df[\"Image\"].map( lambda x : \"/dbfs/tmp/sample/train.zip_files/train/\"+x)\nImageToLabelDict = dict( zip( df[\"Image\"], df[\"Id\"]))"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["SIZE = 64\n#image are imported with a resizing and a black and white conversion\ndef ImportImage( filename):\n    img = Image.open(filename).convert(\"LA\").resize( (SIZE,SIZE))\n    return np.array(img)[:,:,0]\ntrain_img = np.array([ImportImage( img) for img in train_images])\nx = train_img"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["print( \"%d training images\" %x.shape[0])\n\nprint( \"Nbr of samples/class\\tNbr of classes\")\nfor index, val in df[\"Id\"].value_counts().value_counts().sort_index().iteritems():\n    print( \"%d\\t\\t\\t%d\" %(index,val))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["class LabelOneHotEncoder():\n    def __init__(self):\n        self.ohe = OneHotEncoder()\n        self.le = LabelEncoder()\n    def fit_transform(self, x):\n        features = self.le.fit_transform( x)\n        return self.ohe.fit_transform( features.reshape(-1,1))\n    def transform( self, x):\n        return self.ohe.transform( self.la.transform( x.reshape(-1,1)))\n    def inverse_tranform( self, x):\n        return self.le.inverse_transform( self.ohe.inverse_tranform( x))\n    def inverse_labels( self, x):\n        return self.le.inverse_transform( x)\n\ny = list(map(ImageToLabelDict.get, train_images))\nlohe = LabelOneHotEncoder()\ny_cat = lohe.fit_transform(y)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["#use of an image generator for preprocessing and data augmentation\nx = x.reshape( (-1,SIZE,SIZE,1))\ninput_shape = x[0].shape\nx_train = x.astype(\"float32\")\ny_train = y_cat\n\nimage_gen = ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=15,\n    width_shift_range=.15,\n    height_shift_range=.15,\n    horizontal_flip=True)\n\n#training the image preprocessing\nimage_gen.fit(x_train, augment=True)\n\n#visualization of some images out of the preprocessing\naugmented_images, _ = next( image_gen.flow( x_train, y_train.toarray(), batch_size=4*4))"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["Hyperparameter-1"],"metadata":{}},{"cell_type":"code","source":["batch_size = 128\nnum_classes = len(y_cat.toarray()[0])\nepochs = 40\n\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\n\nmodel = Sequential()\nmodel.add(Conv2D(48, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(48, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(48, (5, 5), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.33))\nmodel.add(Flatten())\nmodel.add(Dense(24, activation='relu'))\nmodel.add(Dropout(0.33))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\nmodel.summary()\nmodel.fit_generator(image_gen.flow(x_train, y_train.toarray(), batch_size=batch_size),\n          steps_per_epoch=25,\n          epochs=epochs,\n          verbose=1)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["Hyperparameter-2"],"metadata":{}},{"cell_type":"code","source":["batch_size = 128\nnum_classes = len(y_cat.toarray()[0])\nepochs = 20\n\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\n\nmodel = Sequential()\nmodel.add(Conv2D(48, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(48, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(48, (5, 5), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.33))\nmodel.add(Flatten())\nmodel.add(Dense(24, activation='relu'))\nmodel.add(Dropout(0.33))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\nmodel.summary()\nmodel.fit_generator(image_gen.flow(x_train, y_train.toarray(), batch_size=batch_size),\n          steps_per_epoch=25,\n          epochs=epochs,\n          verbose=1)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["Hyperparameter-3"],"metadata":{}},{"cell_type":"code","source":["batch_size = 256\nnum_classes = len(y_cat.toarray()[0])\nepochs = 40\n\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\n\nmodel = Sequential()\nmodel.add(Conv2D(48, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(48, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(48, (5, 5), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.33))\nmodel.add(Flatten())\nmodel.add(Dense(24, activation='relu'))\nmodel.add(Dropout(0.33))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\nmodel.summary()\nmodel.fit_generator(image_gen.flow(x_train, y_train.toarray(), batch_size=batch_size),\n          steps_per_epoch=25,\n          epochs=epochs,\n          verbose=1)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["batch_size = 512\nnum_classes = len(y_cat.toarray()[0])\nepochs = 60\n\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\n\nmodel = Sequential()\nmodel.add(Conv2D(48, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(48, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(48, (5, 5), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.33))\nmodel.add(Flatten())\nmodel.add(Dense(24, activation='relu'))\nmodel.add(Dropout(0.33))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\nmodel.summary()\nmodel.fit_generator(image_gen.flow(x_train, y_train.toarray(), batch_size=batch_size),\n          steps_per_epoch=25,\n          epochs=epochs,\n          verbose=1)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["###The difference between Deep learning on Spark and Deep learning on tensorflow"],"metadata":{}},{"cell_type":"markdown","source":["Tensorflow will occupy all of the cores locally so using spark is mainly distribute all the work to a distributed environment. There is actually no difference between the result if you apply the model and because of the multi-thread functions, there might lead to some unexpected result. Also, the APIs provided by Spark is not sufficient enough and compare to the tensorflow, it's not so easily understood and require a lot of pre-processing work which is quite confusing and the dataframe is quite different from the commonly used type. The databricks do not provide strong hardware such as GPU to support the training process and will automatically shut down. We can conclude that for simple deep learning, using Spark is not very comfortable, we will apply our model locally or run it on specified machines."],"metadata":{}}],"metadata":{"name":"Assignment3","notebookId":114790859704466},"nbformat":4,"nbformat_minor":0}
