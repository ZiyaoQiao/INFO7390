{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The domain is http://www.ign.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import re\n",
    "import time\n",
    "import xlwt\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pages = set()\n",
    "allExternalLinks = set()\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using pages and allExternalLinks to store the pages, headers are used to avoid some sites which will detect scaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_links(current, domain):\n",
    "    externallinks = {}\n",
    "    for link in current.findAll(\"a\", href=re.compile(\n",
    "            \"(http|ftp|https):\\/\\/[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?\")):\n",
    "        if link.attrs['href'] is not None:\n",
    "            if domain not in link.attrs['href']:\n",
    "                if link.attrs['href'] not in externallinks:\n",
    "                    content = check_avail_and_load(link.attrs['href'])\n",
    "                    externallinks[link.attrs['href']] = content\n",
    "                    print(link.attrs['href'])\n",
    "                    print(content)\n",
    "    return externallinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method to detect all the link exists in the given page and check if it is under the same domain. If not, it will call the method to get the content of that link and store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_avail_and_load(link):\n",
    "    req = request.Request(link, headers=headers)\n",
    "    valid = True\n",
    "    try:\n",
    "        response = urlopen(req)\n",
    "        current = BeautifulSoup(response, \"html.parser\")\n",
    "        content = current.title\n",
    "    except Exception as e:\n",
    "        valid = False\n",
    "        return [\"Null\", valid, \"Null\"]\n",
    "    return [format_html(content), valid, time.strftime(format(\"%Y-%m-%d %X\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the method to check if a link is available to open and load the page. It will return the title of the link, availability and time. If the link is not available, return null instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_html(raw_html):\n",
    "    formation = re.compile('<.*?>')\n",
    "    formatted = re.sub(formation, '', str(raw_html)).strip()\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the method to clean the plain text of the web, it will return only the string inside the tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_external_links(link):\n",
    "    req = request.Request(link, headers=headers)\n",
    "    page = urlopen(req)\n",
    "    html = page.read()\n",
    "    domain = urlparse(link).netloc\n",
    "    domain = domain[4:]\n",
    "    current = BeautifulSoup(html, \"html.parser\")\n",
    "    externallinks = get_links(current, domain)\n",
    "    save(externallinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main method of this web scraper, load the link using request adn urlopen, and handle the content using beautifulsoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(externallinks):\n",
    "    workbook = xlwt.Workbook(encoding='utf=8')\n",
    "    worksheet = workbook.add_sheet('Scraper Result')\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for link in externallinks:\n",
    "        if link not in allExternalLinks:\n",
    "            allExternalLinks.add(link)\n",
    "            content = externallinks[link]\n",
    "            worksheet.write(i, j, label=link)\n",
    "            j = j + 1\n",
    "            worksheet.write(i, j, label=content[0])\n",
    "            j = j + 1\n",
    "            worksheet.write(i, j, label=str(content[1]))\n",
    "            j = j + 1\n",
    "            worksheet.write(i, j, label=content[2])\n",
    "            i = i + 1\n",
    "            j = 0\n",
    "    workbook.save('scraper_result.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the save method, saving the scraper result to a xls document using xlwt package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.youtube.com/ign\n",
      "['YouTube', True, '2018-01-25 19:35:18']\n",
      "https://www.facebook.com/ign\n",
      "['IGN - Home | Facebook', True, '2018-01-25 19:35:20']\n",
      "https://twitter.com/IGN\n",
      "['IGN (@IGN) | Twitter', True, '2018-01-25 19:35:21']\n",
      "https://www.instagram.com/igndotcom\n",
      "['IGN (@igndotcom) • Instagram photos and videos', True, '2018-01-25 19:35:22']\n",
      "https://www.twitch.tv/ign\n",
      "['Twitch', True, '2018-01-25 19:35:22']\n",
      "http://preferences-mgr.truste.com/?type=ziffdavispop&pid=ziffdavis01&aid=ziffdavis01\n",
      "['TrustArc Preference Manager', True, '2018-01-25 19:35:22']\n",
      "https://www.facebook.com/ign/\n",
      "['IGN - Home | Facebook', True, '2018-01-25 19:35:24']\n",
      "https://www.instagram.com/igndotcom/\n",
      "['IGN (@igndotcom) • Instagram photos and videos', True, '2018-01-25 19:35:25']\n",
      "https://r.zdbb.net/u/4s6d\n",
      "['Gaming Monthly Subscription Crate with Geeky Themes | Loot Gaming', True, '2018-01-25 19:35:26']\n",
      "https://r.zdbb.net/u/4o56\n",
      "['Amazon.com: Monster Hunter: World - PlayStation 4 Standard Edition: Video Games', True, '2018-01-25 19:35:28']\n"
     ]
    }
   ],
   "source": [
    "get_all_external_links(\"http://www.ign.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the web scraper, saved in the scraper_result.xls."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
